% !TeX root = main.tex

\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

% Title information
\title{\textbf{Advanced Natural Language Processing} \\

       \large Assignment 2: Intent Classification with Feedforward Neural Network }
\author{Lucia Welther, Matrikelnummer: 835106}
\date{\today}

\begin{document}

\maketitle
\section{Training and Testing on Same Data}
Using the entire dataset for both training and testing can lead to very high
accuracy results. The model probably overfits, which means that it memorizes
the training data rather than learning generalizable patterns.
Therefore, keeping a test set separate from the training data
would result in more realistic model performance.

% ============================================================
\section{ReLU and Sigmoid Advantages/Disadvantages}  % Add a title here
% ============================================================
\subsection{ReLU}
Looking at Figure~\ref{fig:ReLU}, you see that the ReLU function outputs zero for all negative 
inputs and increases linearly for positive inputs, while its derivative is a step function.

Some advantages of ReLU are that it is a simple max$(0, z)$ operation and therefore computationally very effective and inexpensive.
It has a constant gradient of 1 for $z > 0$, which helps to mitigate the vanishing gradient problem compared to activation functions such as sigmoid or tanh.
ReLU is also fast in comparison to other activation functions.

On the other hand, ReLU has some disadvantages. The dying ReLU problem occurs when updates get stuck outputting 0 if $z \le 0$ during training.
Once the gradient becomes 0, the neuron can stop updating and may not recover.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{ReLuandItsDerivative.png}
    \caption{ReLU function and its derivative. The function outputs $\max(0, z)$, and the derivative is 1 for $z > 0$ and 0 otherwise.}
    \label{fig:ReLU}
\end{figure}

\subsection{Sigmoid Function}
In Figure~\ref{fig:Sigmoid}, you see the sigmoid function as a S-shaped curve with outputs 
reaching from 0 to 1, while its derivative 
forms a bell-shaped curve that peaks at $z = 0$ and vanishes for large $|z|$.

Unlike ReLU, sigmoid is continuous and therefore differentiable everywhere. This ensures stable gradients
for small input values. Sigmoid also always outputs a value between $[0,1]$, even when 
$z \le 0$, making it well suited for modeling probabilities.

Sigmoid has the disadvantage of the vanishing gradient problem: when $|z|$ gets too large, the gradient
gets very close to zero.This results in exponentially shrinking gradients during backpropagation, 
which slows down the learning in deeper networks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{SigmoidandItsDerivative.png}
    \caption{Sigmoid function and its derivative. The function outputs values in $(0, 1)$, and the derivative peaks at $z = 0$ and approaches zero for large $|z|$.}
    \label{fig:Sigmoid}
\end{figure}



\section{Batch and Mini-Batch Gradient Descent Loss}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{batch_gd_loss.png}
        \caption{Batch Gradient Descent}
        \label{fig:batch_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{minibatch_gd_loss.png}
        \caption{Mini-batch Gradient Descent (batch\_size=64)}
        \label{fig:minibatch_loss}
    \end{subfigure}
    \caption{Training loss over 1000 epochs for batch GD and mini-batch GD.}
    \label{fig:loss_comparison}
\end{figure}

\subsection{Observation: Batch GD}

The loss decreases from more than 10 to 0.98. 
During the first epochs, the loss drops the most, followed by a smooth convergence.
The accuracy starts at 12.2\% and improves
to 75.40\%, which shows that the network was trained successfully.

\subsection{Observation: Mini-Batch GD}

With mini-batch training, the loss decreases even more dramatically than with Batch GD.
From a loss of 10.0 it decreases to 0.04, where the first 100 epochs show the 
most drastic drop from 10.0 to 0.59, followed by a continued steady decrease throughout training.
The accuracy, same as for batch GD, starts at 12.2\%, representing random guessing.
Mini-batch improves to 99.30\%, which is significantly higher than the accuracy from
batch GD. Mini-batch has an increased number of weight
updates per epoch, which allows the network to learn more effectively.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{sgd_loss.png}
    \caption{Training loss for SGD (batch\_size=1) over 1000 epochs.}
    \label{fig:sgd_loss}
\end{figure}

\subsection{Observation: Stochastic GD}

SGD shows the most drastic loss decrease within the first epochs compared to the other gradient descent methods. 
Starting with a loss of 3 and dropping to close to zero within the first few epochs, this indicates 
that the model overfits to the training set.
The accuracy reaches 99.9\%, which is unusually high and confirms overfitting.
This is due to testing on the same data used for training.


\section{GitHub Repository}

GitHub Repository: \url{https://github.com/luzecode/aNLP_Assignment2.git}

\textbf{Last commit hash:} \texttt{Added Report and Jupyter Notebook for Plots, some adjustments to the Code}




\end{document}